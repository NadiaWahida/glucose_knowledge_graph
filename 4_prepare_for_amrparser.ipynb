{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remember our Topic Clustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded topics2storyID mapping into memory\n",
      "\n",
      "loaded top 15 topic words into memory\n"
     ]
    }
   ],
   "source": [
    "with open('../story_graphs/topic2storyID.json', 'r') as f:\n",
    "    topics = json.load(f)\n",
    "print('loaded topics2storyID mapping into memory\\n')\n",
    "    \n",
    "with open('../story_cluster/clustering_results_min_15_top_words.json', 'r') as f:\n",
    "    top_words = json.load(f)\n",
    "print('loaded top 15 topic words into memory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CLUSTER](../story_cluster/story_cluster.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID:  -1 \n",
      "\n",
      "went\n",
      "day\n",
      "got\n",
      "decided\n",
      "friends\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  0 \n",
      "\n",
      "zoo\n",
      "animals\n",
      "monkey\n",
      "lion\n",
      "kids\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  1 \n",
      "\n",
      "dog\n",
      "puppy\n",
      "dogs\n",
      "pet\n",
      "ran\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  2 \n",
      "\n",
      "coffee\n",
      "cup\n",
      "drink\n",
      "lid\n",
      "spilled\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  3 \n",
      "\n",
      "dentist\n",
      "shave\n",
      "tooth\n",
      "beard\n",
      "teeth\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  4 \n",
      "\n",
      "snow\n",
      "cold\n",
      "outside\n",
      "winter\n",
      "ice\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  5 \n",
      "\n",
      "tree\n",
      "wood\n",
      "squirrel\n",
      "build\n",
      "yard\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  6 \n",
      "\n",
      "cat\n",
      "kitten\n",
      "cats\n",
      "kittens\n",
      "sarah\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  7 \n",
      "\n",
      "cd\n",
      "song\n",
      "music\n",
      "headphones\n",
      "listen\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  8 \n",
      "\n",
      "flu\n",
      "medicine\n",
      "better\n",
      "doctor\n",
      "feeling\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  9 \n",
      "\n",
      "sleep\n",
      "asleep\n",
      "night\n",
      "fell\n",
      "moon\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  10 \n",
      "\n",
      "bike\n",
      "car\n",
      "tire\n",
      "driving\n",
      "riding\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  11 \n",
      "\n",
      "clean\n",
      "clothes\n",
      "trash\n",
      "laundry\n",
      "wash\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  12 \n",
      "\n",
      "toy\n",
      "birthday\n",
      "wagon\n",
      "party\n",
      "billy\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  13 \n",
      "\n",
      "apple\n",
      "banana\n",
      "apples\n",
      "tree\n",
      "fruit\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  14 \n",
      "\n",
      "team\n",
      "basketball\n",
      "game\n",
      "play\n",
      "day\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  15 \n",
      "\n",
      "store\n",
      "list\n",
      "grocery\n",
      "groceries\n",
      "needed\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  16 \n",
      "\n",
      "new\n",
      "pair\n",
      "shoes\n",
      "pants\n",
      "doll\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  17 \n",
      "\n",
      "cookies\n",
      "oven\n",
      "bake\n",
      "dough\n",
      "baking\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  18 \n",
      "\n",
      "cake\n",
      "birthday\n",
      "candles\n",
      "jenny\n",
      "blow\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  19 \n",
      "\n",
      "garden\n",
      "plant\n",
      "tomato\n",
      "vegetables\n",
      "seeds\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  20 \n",
      "\n",
      "ball\n",
      "baseball\n",
      "dad\n",
      "playing\n",
      "son\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  21 \n",
      "\n",
      "dance\n",
      "fun\n",
      "dancing\n",
      "party\n",
      "friends\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  22 \n",
      "\n",
      "beach\n",
      "sand\n",
      "vacation\n",
      "waves\n",
      "day\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  23 \n",
      "\n",
      "anna\n",
      "late\n",
      "work\n",
      "quickly\n",
      "boss\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  24 \n",
      "\n",
      "bus\n",
      "work\n",
      "pm\n",
      "stop\n",
      "late\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  25 \n",
      "\n",
      "cream\n",
      "ice\n",
      "soda\n",
      "juice\n",
      "cone\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  26 \n",
      "\n",
      "jean\n",
      "ate\n",
      "sandwich\n",
      "restaurant\n",
      "subway\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  27 \n",
      "\n",
      "chicken\n",
      "rice\n",
      "make\n",
      "spicy\n",
      "sauce\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  28 \n",
      "\n",
      "pizza\n",
      "cook\n",
      "food\n",
      "hungry\n",
      "oven\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  29 \n",
      "\n",
      "fishing\n",
      "fish\n",
      "lake\n",
      "caught\n",
      "pole\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  30 \n",
      "\n",
      "swim\n",
      "lizard\n",
      "fish\n",
      "water\n",
      "pool\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  31 \n",
      "\n",
      "kicked\n",
      "didn\n",
      "val\n",
      "richard\n",
      "ali\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  32 \n",
      "\n",
      "test\n",
      "teacher\n",
      "class\n",
      "homework\n",
      "school\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  33 \n",
      "\n",
      "gina\n",
      "mom\n",
      "home\n",
      "mother\n",
      "house\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  34 \n",
      "\n",
      "friends\n",
      "new\n",
      "nervous\n",
      "school\n",
      "kids\n",
      "\n",
      "--------------------\n",
      "\n",
      "ID:  35 \n",
      "\n",
      "date\n",
      "met\n",
      "girl\n",
      "dating\n",
      "dance\n",
      "\n",
      "--------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_words = {int(key):val for (key,val) in top_words.items()}\n",
    "for (topic,words) in sorted(top_words.items()):\n",
    "    print('ID: ', topic,'\\n')\n",
    "    for word in words[:5]:\n",
    "        print(word[0])\n",
    "    print('\\n'+'-'*20+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "animal_related_cluster = ['0','1','6']\n",
    "fish_cluster = ['22','29','30']\n",
    "food_cluster = ['13','17','19','25','27','28']\n",
    "other_cluster = ['5','10','4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graphs_per_cluster(cluster):\n",
    "    files = topics[cluster]\n",
    "    graphs = []\n",
    "    for file in files:\n",
    "        with open('../story_graphs/'+file, 'rb') as f:\n",
    "            graphs.append(pickle.load(f))\n",
    "    return(graphs)\n",
    "            \n",
    "#zoo_graphs = get_graphs_per_cluster('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph = zoo_graphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_graph_info(graph):\n",
    "    print('The graph has the following ', len(graph.nodes), ' nodes:\\n')\n",
    "    for node in graph.nodes:\n",
    "        print(node)\n",
    "        print(graph.nodes[node])\n",
    "        print('\\n')\n",
    "    print('\\nThe graph has the following ', len(graph.edges), ' edges:\\n')\n",
    "    for edge in graph.edges:\n",
    "        print(edge)\n",
    "        print(graph.edges[edge])\n",
    "        print('\\n')\n",
    "#print_graph_info(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to parse?\n",
    "\n",
    "1. Story Sentences\n",
    "2. Specific Rule\n",
    "3. Replaced General Rule without brackets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_2parse(graph):\n",
    "    specific = []\n",
    "    general = []\n",
    "    for node in graph.nodes:\n",
    "        if '_S' in node:\n",
    "            specific += graph.nodes[node]['2parse']\n",
    "    for edge in graph.edges:\n",
    "        if '_S' in edge[0]:\n",
    "            continue\n",
    "        for annotation in graph.edges[edge]['annotations']:\n",
    "            if None not in annotation['2parse']:\n",
    "                specific += [annotation['2parse'][0][0], annotation['2parse'][0][2]]\n",
    "                general += [annotation['2parse'][1][0], annotation['2parse'][1][2]]\n",
    "    specific = list(set(specific))\n",
    "    general = list(set(general))\n",
    "    \n",
    "    return(specific, general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_2parse_file(graphs, cluster):\n",
    "    parse = []\n",
    "    for graph in graphs:\n",
    "        specific, general = collect_2parse(graph)\n",
    "        parse += specific + general\n",
    "    len_before = len(parse)\n",
    "    parse = list(set(parse))\n",
    "    #print(len_before, len(parse))\n",
    "    print(len_before - len(parse) ,' reductions in cluster ', cluster)\n",
    "    \n",
    "    with open('../amr/2parse/c_'+cluster+'.txt', 'w') as f:\n",
    "        for sent in parse:\n",
    "            f.writelines(sent+'\\n')\n",
    "    return(parse)\n",
    "            \n",
    "#mk_2parse_file(zoo_graphs, '0')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parses(clusters):\n",
    "    parses = []\n",
    "    for cluster in clusters:\n",
    "        graphs = get_graphs_per_cluster(cluster)\n",
    "        parse = mk_2parse_file(graphs, cluster)\n",
    "        parses += parse\n",
    "    len_before = len(parses)\n",
    "    parses = list(set(parses))\n",
    "    print(len_before - len(parses) ,' reductions in animal related clusters')\n",
    "    print(len(parses),' sentences to parse in animal related clusters')\n",
    "    return(parses)\n",
    "#parses = get_parses(fish_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking for paralell parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(parses, chunk_size, name):\n",
    "\n",
    "    chunks = []\n",
    "    file_len = len(parses)//chunk_size + 1\n",
    "    print(file_len)\n",
    "    print(file_len*chunk_size)\n",
    "    last = 0\n",
    "    while last<len(parses):\n",
    "        chunks += [parses[last:last+file_len]]\n",
    "        #chunks.append([parses[last:last+file_len]])\n",
    "        last += file_len\n",
    "    #chunks.append([parses[last:]])\n",
    "    print(len(chunks))\n",
    "    #print(chunks[0])\n",
    "    for chunk in chunks:\n",
    "        new_name = name + '_chunk_'+str(chunks.index(chunk))+'.txt'\n",
    "        with open('../amr/2parse/'+new_name, 'w') as f:\n",
    "            for sent in chunk:\n",
    "                #print(sent)\n",
    "                f.writelines(sent+'\\n')\n",
    "        \n",
    "#chunk(parses, 5, 'animal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300  reductions in cluster  13\n",
      "602  reductions in cluster  17\n",
      "314  reductions in cluster  19\n",
      "487  reductions in cluster  25\n",
      "454  reductions in cluster  27\n",
      "1207  reductions in cluster  28\n",
      "1701  reductions in animal related clusters\n",
      "25427  sentences to parse in animal related clusters\n",
      "5086\n",
      "25430\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "parses = get_parses(food_cluster)\n",
    "chunk(parses, 5, 'food')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
